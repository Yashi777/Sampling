# -*- coding: utf-8 -*-
"""Sampling_102003374

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FocF5kvPcaOtWh3h4wsAB4-0hOGWHcoL

###Uploading Dataset
"""

from google.colab import files
upload = files.upload()

"""###Importing Libraries"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score   
import random

"""###Pre-Processing on Dataset"""

dataset = pd.read_csv('Creditcard_data.csv')

dataset.head()

dataset['Class'].value_counts()

x = dataset.loc[:,dataset.columns!='Class']
y = dataset.loc[:,dataset.columns=='Class']

"""####Over-sampling as dataset is imbalanced"""

from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=42)
x_ros, y_ros = ros.fit_resample(x, y)

y_ros.value_counts()

x_ros['Class'] = y_ros

x_ros

"""###Sampling 1 - Simple Random Sampling"""

# Simple Random Sampling
simp_sample = x_ros.sample(n = 1067, random_state=42)

# x and y division
x = simp_sample.loc[:,simp_sample.columns!='Class']
y = simp_sample.loc[:,simp_sample.columns=='Class']

# Training-Testing Splitting
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)

# Logistic Regression
from sklearn import linear_model
logr = linear_model.LogisticRegression()
logr.fit(x_train, y_train)
logr_pred = logr.predict(x_test)
score = accuracy_score(logr_pred, y_test)
print(score)

# Decision Tree Classifier
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()
dtree = dtree.fit(x_train, y_train)
dtree_pred = dtree.predict(x_test)
score = accuracy_score(dtree_pred, y_test)
print(score)

# rf
from sklearn.ensemble import RandomForestClassifier 
randfor = RandomForestClassifier(n_estimators=1, random_state=0)
randfor.fit(x, y)
rf_prediction = randfor.predict(x_test)
score = accuracy_score(rf_prediction, y_test)
print(score)

# Gaussian Naive Bayes
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(x_train, y_train)
gnb_pred = gnb.predict(x_test)
score = accuracy_score(gnb_pred, y_test)
print(score)

# KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
knn_pred = knn.predict(x_test)
score = accuracy_score(knn_pred, y_test)
print(score)

# Sampling 1 - Simple Random Sampling
sampling1 = [accuracy_score(y_test,logr_pred)*100,accuracy_score(y_test,dtree_pred)*100,
                                          accuracy_score(rf_prediction, y_test)*100,accuracy_score(y_test,gnb_pred)*100,
                                          accuracy_score(y_test,knn_pred)*100]

conclusion1 = pd.DataFrame({'Models': ["Logistic Regression","Decision Tree","SVM","Gaussian Naive Bayes","KNN"],
                           'Accuracies': [accuracy_score(y_test,logr_pred)*100,accuracy_score(y_test,dtree_pred)*100,
                                          accuracy_score(rf_prediction, y_test)*100,accuracy_score(y_test,gnb_pred)*100,
                                          accuracy_score(y_test,knn_pred)*100]})
conclusion1.sort_values('Accuracies', ascending=False).reset_index(drop=True)

"""###Sampling 2 - Stratified Sampling"""

# Stratified Sampling
strat_sample = x_ros.groupby('Class', group_keys=False).apply(lambda x: x.sample(392,replace=True,random_state=42))

# x and y division
x = strat_sample.loc[:,strat_sample.columns!='Class']
y = strat_sample.loc[:,strat_sample.columns=='Class']

# Training-Testing Splitting
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)

# Logistic Regression
from sklearn import linear_model
logr = linear_model.LogisticRegression()
logr.fit(x_train, y_train)
logr_pred = logr.predict(x_test)
score = accuracy_score(logr_pred, y_test)
print(score)

# Decision Tree Classifier
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()
dtree = dtree.fit(x_train, y_train)
dtree_pred = dtree.predict(x_test)
score = accuracy_score(dtree_pred, y_test)
print(score)

from sklearn.ensemble import RandomForestClassifier 
randfor = RandomForestClassifier(n_estimators=1, random_state=0)
randfor.fit(x, y)
rf_prediction = randfor.predict(x_test)
score = accuracy_score(rf_prediction, y_test)
print(score)

# Gaussian Naive Bayes
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(x_train, y_train)
gnb_pred = gnb.predict(x_test)
score = accuracy_score(gnb_pred, y_test)
print(score)

# KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
knn_pred = knn.predict(x_test)
score = accuracy_score(knn_pred, y_test)
print(score)

# Sampling 2 - Stratified Sampling
sampling2 = [accuracy_score(y_test,logr_pred)*100,accuracy_score(y_test,dtree_pred)*100,
                                         accuracy_score(rf_prediction, y_test)*100,accuracy_score(y_test,gnb_pred)*100,
                                          accuracy_score(y_test,knn_pred)*100]

conclusion2 = pd.DataFrame({'Models': ["Logistic Regression","Decision Tree","SVM","Gaussian Naive Bayes","KNN"],
                           'Accuracies': [accuracy_score(y_test,logr_pred)*100,accuracy_score(y_test,dtree_pred)*100,
                                          accuracy_score(rf_prediction, y_test)*100,accuracy_score(y_test,gnb_pred)*100,
                                          accuracy_score(y_test,knn_pred)*100]})
conclusion2.sort_values('Accuracies', ascending=False).reset_index(drop=True)

"""###Sampling 3 - Systematic Sampling"""

# Systematic Sampling
sys_sample_df = x_ros.iloc[::5]

# x and y division
x = sys_sample_df.loc[:,sys_sample_df.columns!='Class']
y = sys_sample_df.loc[:,sys_sample_df.columns=='Class']

# Training-Testing Splitting
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)

# Logistic Regression
from sklearn import linear_model
logr = linear_model.LogisticRegression()
logr.fit(x_train, y_train)
logr_pred = logr.predict(x_test)
score = accuracy_score(logr_pred, y_test)
print(score)

# Decision Tree Classifier
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()
dtree = dtree.fit(x_train, y_train)
dtree_pred = dtree.predict(x_test)
score = accuracy_score(dtree_pred, y_test)
print(score)

from sklearn.ensemble import RandomForestClassifier 
randfor = RandomForestClassifier(n_estimators=1, random_state=0)
randfor.fit(x, y)
rf_prediction = randfor.predict(x_test)
score = accuracy_score(rf_prediction, y_test)
print(score)

# Gaussian Naive Bayes
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(x_train, y_train)
gnb_pred = gnb.predict(x_test)
score = accuracy_score(gnb_pred, y_test)
print(score)

# KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
knn_pred = knn.predict(x_test)
score = accuracy_score(knn_pred, y_test)
print(score)

# Sampling 3 - Systematic Sampling
sampling3 = [accuracy_score(y_test,logr_pred)*100,accuracy_score(y_test,dtree_pred)*100,
                                         accuracy_score(rf_prediction, y_test)*100,accuracy_score(y_test,gnb_pred)*100,
                                          accuracy_score(y_test,knn_pred)*100]

conclusion3 = pd.DataFrame({'Models': ["Logistic Regression","Decision Tree","SVM","Gaussian Naive Bayes","KNN"],
                           'Accuracies': [accuracy_score(y_test,logr_pred)*100,accuracy_score(y_test,dtree_pred)*100,
                                         accuracy_score(rf_prediction, y_test)*100,accuracy_score(y_test,gnb_pred)*100,
                                          accuracy_score(y_test,knn_pred)*100]})
conclusion3.sort_values('Accuracies', ascending=False).reset_index(drop=True)

"""###Sampling 4 - Cluster Sampling"""

# Cluster Sampling
def get_clustered_Sample(df, n_per_cluster, num_select_clusters):
    N = len(df)
    K = int(N/n_per_cluster)
    data = None
    for k in range(K):
        sample_k = df.sample(n_per_cluster)
        sample_k["cluster"] = np.repeat(k,len(sample_k))
        df = df.drop(index = sample_k.index)
        data = pd.concat([data,sample_k],axis = 0)
    random_chosen_clusters = np.random.randint(0,K,size = num_select_clusters)
    samples = data[data.cluster.isin(random_chosen_clusters)]
    return(samples)
clust_sample = get_clustered_Sample(df = x_ros, n_per_cluster = 20, num_select_clusters = 28)

# x and y division
x = clust_sample.loc[:,clust_sample.columns!='Class']
y = clust_sample.loc[:,clust_sample.columns=='Class']

# Training-Testing Splitting
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)

# Logistic Regression
from sklearn import linear_model
logr = linear_model.LogisticRegression()
logr.fit(x_train, y_train)
logr_pred = logr.predict(x_test)
score = accuracy_score(logr_pred, y_test)
print(score)

# Decision Tree Classifier
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()
dtree = dtree.fit(x_train, y_train)
dtree_pred = dtree.predict(x_test)
score = accuracy_score(dtree_pred, y_test)
print(score)

from sklearn.ensemble import RandomForestClassifier 
randfor = RandomForestClassifier(n_estimators=1, random_state=0)
randfor.fit(x, y)
rf_prediction = randfor.predict(x_test)
score = accuracy_score(rf_prediction, y_test)
print(score)

# Gaussian Naive Bayes
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(x_train, y_train)
gnb_pred = gnb.predict(x_test)
score = accuracy_score(gnb_pred, y_test)
print(score)

# KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
knn_pred = knn.predict(x_test)
score = accuracy_score(knn_pred, y_test)
print(score)

# Sampling 4 - Cluster Sampling
sampling4 = [accuracy_score(y_test,logr_pred)*100,accuracy_score(y_test,dtree_pred)*100,
                                         accuracy_score(rf_prediction, y_test)*100,accuracy_score(y_test,gnb_pred)*100,
                                          accuracy_score(y_test,knn_pred)*100]

conclusion4 = pd.DataFrame({'Models': ["Logistic Regression","Decision Tree","SVM","Gaussian Naive Bayes","KNN"],
                           'Accuracies': [accuracy_score(y_test,logr_pred)*100,accuracy_score(y_test,dtree_pred)*100,
                                          accuracy_score(rf_prediction, y_test)*100,accuracy_score(y_test,gnb_pred)*100,
                                          accuracy_score(y_test,knn_pred)*100]})
conclusion4.sort_values('Accuracies', ascending=False).reset_index(drop=True)

"""###Sampling 5 - Multi-Stage Sampling (Cluster and Simple Random)"""

# Multi-Stage Sampling (Cluster sampling then simple random sampling)
def get_clustered_Sample(df, n_per_cluster, num_select_clusters):
    N = len(df)
    K = int(N/n_per_cluster)
    data = None
    for k in range(K):
        sample_k = df.sample(n_per_cluster)
        sample_k["cluster"] = np.repeat(k,len(sample_k))
        df = df.drop(index = sample_k.index)
        data = pd.concat([data,sample_k],axis = 0)
    random_chosen_clusters = np.random.randint(0,K,size = num_select_clusters)
    samples = data[data.cluster.isin(random_chosen_clusters)]
    return(samples)
clust_sample = get_clustered_Sample(df = x_ros, n_per_cluster = 20, num_select_clusters = 20)
mix_sample = clust_sample.sample(n = 200, random_state=42)

# x and y division
x = mix_sample.loc[:,mix_sample.columns!='Class']
y = mix_sample.loc[:,mix_sample.columns=='Class']

# Training-Testing Splitting
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)

# Logistic Regression
from sklearn import linear_model
logr = linear_model.LogisticRegression()
logr.fit(x_train, y_train)
logr_pred = logr.predict(x_test)
score = accuracy_score(logr_pred, y_test)
print(score)

# Decision Tree Classifier
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()
dtree = dtree.fit(x_train, y_train)
dtree_pred = dtree.predict(x_test)
score = accuracy_score(dtree_pred, y_test)
print(score)

from sklearn.ensemble import RandomForestClassifier 
randfor = RandomForestClassifier(n_estimators=1, random_state=0)
randfor.fit(x, y)
rf_prediction = randfor.predict(x_test)
score = accuracy_score(rf_prediction, y_test)
print(score)

# Gaussian Naive Bayes
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(x_train, y_train)
gnb_pred = gnb.predict(x_test)
score = accuracy_score(gnb_pred, y_test)
print(score)

# KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
knn_pred = knn.predict(x_test)
score = accuracy_score(knn_pred, y_test)
print(score)

# Sampling 5 - Multi-Stage Sampling
sampling5 = [accuracy_score(y_test,logr_pred)*100,accuracy_score(y_test,dtree_pred)*100,
                                        accuracy_score(rf_prediction, y_test)*100,accuracy_score(y_test,gnb_pred)*100,
                                          accuracy_score(y_test,knn_pred)*100]

conclusion5 = pd.DataFrame({'Models': ["Logistic Regression","Decision Tree","SVM","Gaussian Naive Bayes","KNN"],
                           'Accuracies': [accuracy_score(y_test,logr_pred)*100,accuracy_score(y_test,dtree_pred)*100,
                                         accuracy_score(rf_prediction, y_test)*100,accuracy_score(y_test,gnb_pred)*100,
                                          accuracy_score(y_test,knn_pred)*100]})
conclusion5.sort_values('Accuracies', ascending=False).reset_index(drop=True)

"""### Final Result Table"""

models = ["Logistic Regression","Decision Tree","RF","Gaussian Naive Bayes","KNN"]
final_table = pd.DataFrame()
final_table['Models'] = models
final_table['Simple Random'] = sampling1
final_table['Stratified'] = sampling2
final_table['Systematic'] = sampling3
final_table['Cluster'] = sampling4
final_table['Multi-Stage'] = sampling5

final_table

"""On comparison, we can see that when we apply `RF Algorithm` on `cluster sampling `, we get the highest accuracy."""